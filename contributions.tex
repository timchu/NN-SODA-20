\subsection{Contributions and Past Work}
Besides for exactly computing the nearest neighbor metric, we present the
following theorems on approximate computation:

 \input{spanner-theorems}
These theorems rely on Theorem~\ref{thm:NN} and considerably strengthen the
spanner resluts on the Nearest Neighbor metric
from~\cite{cohen15approximating}. Theorem~\ref{thm:NN} will also allow us
to compute the persistent homology of $\dist_N$.

Theorem~\ref{thm:general-spanner} proves that a $(1+\eps)$-spanner of the
edge-squared metric with points in constant dimension is sparser and can be
computed more efficiently than the theoretical optimal Euclidean
spanner~\cite{}.  
Previously, sparse spanners of the edge-squared metric were shown to exist in two
dimensions via Yao graphs and Gabriel graphs~\cite{LiWan2001}, but these
did not generalize well to constant dimension: Yao
graphs are not very efficient to compute, and Gabriel graphs can have
quadratically many edges even in $3$ dimension.

Theorem~\ref{thm:distribution-spanner} proves that a $1$-spanner of
the edge-squared metric can be found assuming points are samples from a
probability density, by using a $k$-$NN$ graph for
appropriate $k$. Our result is tight when $d$ is constant. This
is not possible for Euclidean distance, as a $1$-spanner is almost
surely the complete graph. Without the probability density
assumption, there are point sets in $\mathbb{R}^4$ where
$1$-spanners
of the edge-squared metric require $\Omega(n^2)$ edges.  
Finally, we show how the nearest neighbor metric generalizes Euclidean
distance and maximum-edge Euclidean MST distance ~\cite{LiWan2001}

The core mathematical contribution of our work should be considered our
proof of Theorem~\ref{thm:NN}, which we believe to be surprising for
reasons previously mentioned.

%% Old outline: check this outline before submitting.
%% \begin{enumerate}
%% \item Definition of edge-squared.
%% \item Preliminaries
%% \item Outline/Overview/Previous Work
%% \item Interpretations based on known Machine Learning tools.
%% \begin{enumerate}
%% \item Gaussian Kernel similarity.
%% \item Generalization of Level-Set and Single-Linkage clustering. (\tim{This
%%     is not a very good point.})
%% \item Interpretation as $l_2$ on paths?
%% \end{enumerate}
%% \item Equality to a natural geodesic distance.
%% \begin{enumerate}
%% \item Define NN-metric.
%% \item Core Proof.
%% \item NN has a fast sparse spanner. Any spanner of the NN metric is a
%% spanner of edge-squared. (This result is theoretically
%%     superseded by our later result, but is of independent interest).
%% \item Persistent homology of the Nearest Neighbor metric can be
%% computed.
%% \end{enumerate}
%% \item Fast practical spanners for points in a distribution. ($k$-NN
%%     graph for $k = O( \log n)$.
%% \item Theoretically sparse, fast spanners for points in low dimension.  \tim{Low intrinsic dimension? Can cover trees do this for me?}
%% \item Remaining open questions.
%% \item Appendix:
%% \begin{enumerate}
%% \item Persistent homology can be put here, maybe.
%% \item Links to Heirarchies of Metrics: Edge-Squared is a natural
%% extension of negative type metrics, with potentially high doubling dimension.
%% \end{enumerate}
%%
%% \end{enumerate}
