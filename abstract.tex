\begin{abstract}

  % Basic Object and main result
  Data-sensitive metrics adapt distances locally based the density of data points with the goal of aligning distances and some notion of similarity.
  In this paper, we give the first exact algorithm for computing a data-sensitive metric called the Nearest Neighbor Metric.
  In fact, we prove the surprising result that a previously published $3$-approximation is an exact algorithm.

  % Surprising and Hard
  The Nearest Neighbor Metric can be viewed as a special case of a density-based distance used in machine learning, or it can be seen as an example of a manifold metric.
  Previous computational research on such metrics despaired of computing exact distances on account of the apparent difficulty of minimizing over all continuous paths between a pair of points.

  % ancillary results
  We also compute a generalization of this metric, which we call the $q$-power Nearest Neighbor metric, and give an analogous graph-based metric for point sets that are the union of 4 compact, path-connected sets in arbitrary dimension.
  We leverage the exact computation of the Nearest Neighbor Metric to compute sparse spanners and persistent homology.
  We also explore the behavior of the metric built from point sets drawn from an underlying distribution and consider the more general case of inputs that are countable collections of compact sets.

  % Interesting Connections
  The main results connect several classical theories such as the conformal change of Riemannian metrics and the screw theory of Schoenberg and Von Neumann.
  We also develop some novel proof techniques based on minimum cost flows and Lipschitz extensions that may be of independent interest.

\end{abstract}
